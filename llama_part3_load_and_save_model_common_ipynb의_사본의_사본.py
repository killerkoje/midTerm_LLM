# -*- coding: utf-8 -*-
"""llama_part3_Load and Save Model_common.ipynb의 사본의 사본

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ai04GFA3DgnrNZ8jaVokhsGKPWkqbJY1
"""

# savePath = "/content/gdrive/MyDrive/Colab Notebooks/data_edu/16. LLM 파인튜닝/models/llama_tune_{now_str}"

try:
    import google.colab
    inColab = True
except ImportError:
    inColab = False

inColab

if inColab == True:
    !pip install -U pandas==2.2.2 numpy==2.0.2 scipy==1.14.1 accelerate==1.6.0 peft==0.15.2 bitsandbytes==0.45.5 transformers==4.51.3 trl==0.16.1 datasets==3.5.0 tensorboard==2.19.0

from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    pipeline,
    logging
)
from transformers import AutoConfig,AutoModel
import torch
from peft import PeftModel, PeftConfig

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/gdrive/MyDrive/김효관교수님/김효관교수님/Colab Notebooks/fintech_edu_2025/4.LLM_파인튜닝2025_05_02_09

if inColab == True:
    from google.colab import drive
    drive.mount("/content/gdrive")

import huggingface_hub
huggingface_hub.login("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")

"""# 1. Load base and new model"""

# from transformers import AutoConfig, AutoModel, AutoTokenizer
# config = AutoConfig.from_pretrained("your model name", revision=revision)
# model = AutoModel.from_pretrained("your model name", revision=revision)
# tokenizer = AutoTokenizer.from_pretrained("your model name", revision=revision)

"""### ★★★ 수정 포인트 ★★★"""

## base 모델
base_model = "meta-llama/Meta-Llama-3.1-8B-Instruct"
## 파인튜닝한 모델 저장 위치 설정
new_model = "/content/gdrive/MyDrive/김효관교수님/김효관교수님/Colab Notebooks/fintech_edu_2025/4.LLM_파인튜닝2025_05_02_09"

### 베이스모델 불러오기
baseModel = AutoModelForCausalLM.from_pretrained(
    base_model,
    low_cpu_mem_usage=True,
    return_dict=True,
    torch_dtype=torch.float16,
    # device_map= "auto"
    device_map= {"": 0}
)

### 토크나이저 불러오기
tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

# PEFT 설정 로드
peft_config = PeftConfig.from_pretrained(new_model)
print(peft_config)

# 베이스모델에 어댑터 적용
model = PeftModel.from_pretrained(baseModel, new_model)

"""### 어뎁터 연결 모델 정상작동 확인"""

pipe = pipeline(
    task="text-generation",
    model=model,
    tokenizer=tokenizer,
    model_kwargs={"torch_dtype": torch.float16},
    truncation=True
)
def extract_response_llama3(question):
    messages = [
        {"role": "system", "content": ""},
        {"role": "user", "content": question},
    ]

    prompt = pipe.tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    terminators = [
        pipe.tokenizer.eos_token_id,
        pipe.tokenizer.convert_tokens_to_ids("<|eot_id|>")
    ]

    outputs = pipe(
        prompt,
        max_new_tokens=256,
        eos_token_id=terminators,
        do_sample=True,
        temperature=0.1,
        top_p=0.9,
        num_return_sequences=1
    )

    generated_text = outputs[0]['generated_text']
    response_lines = generated_text.strip().split('\n')
    meaningful_response = response_lines[-1]

    return meaningful_response
question = "다테 기미코가 최초로 은퇴 선언을 한게 언제지"
response = extract_response_llama3(question)
print(response)

# 어댑터 병합
mergedModel = model.merge_and_unload()

# set your HF repository
hfAddr = "LEEHEEWON/midTermModel"

# save model and tokenizer to HF hub
mergedModel.push_to_hub(hfAddr)
tokenizer.push_to_hub(hfAddr)

"""★ 참고 코드"""

### 리더보드 올리기전 테스트 코드 (모델 이상여부 확인, 에러 시 등록 X)

from transformers import AutoConfig, AutoModel, AutoTokenizer
hfAddr = "hyokwan/llama31_common"
config = AutoConfig.from_pretrained(hfAddr, revision="main")
model = AutoModel.from_pretrained(hfAddr, revision="main")
tokenizer = AutoTokenizer.from_pretrained(hfAddr, revision="main")

