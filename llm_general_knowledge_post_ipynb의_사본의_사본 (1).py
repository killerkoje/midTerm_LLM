# -*- coding: utf-8 -*-
"""LLM_GENERAL_KNOWLEDGE_POST.ipynb의 사본의 사본

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jKespcwNGoxskYyeXgE36FOx1WXeN7DA
"""

!pip install pymysql
!pip install uvicorn fastapi
!pip install  nest-asyncio pyngrok

"""### 라이브러리 선언"""

# Pandas 패키지 불러오기
import pandas as pd
from sqlalchemy import create_engine, inspect

pip show transformers

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# 서버 관리용 fastapi 의존 라이브러리
import uvicorn

# fast api 라이브러리
from fastapi import FastAPI

# 머신러닝 모델 관리용 라이브러리
# import pickle

# 데이터프레임 및 수 처리 라이브러리
import pandas as pd
import numpy as np
# 인터페이스 데이터 관리를 위한 라이브러리
from pydantic import BaseModel

import nest_asyncio
from pyngrok import ngrok
import uvicorn

from fastapi.middleware.cors import CORSMiddleware
origins = ["*"]

app = FastAPI(title="GENERAL_KNOWLEDGE API")

# CORS 미들웨어 추가
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # 모든 origin 허용
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE"],
    allow_headers=["*"],
)

"""### 2. 모델 불러오기"""

## base 모델
base_model = "LEEHEEWON/midTermModel"
### 베이스모델 불러오기
baseModel = AutoModelForCausalLM.from_pretrained(
    base_model,
    low_cpu_mem_usage=True,
    return_dict=True,
    torch_dtype=torch.float16,
    device_map= "auto" # T4 GPU 사용 시
    # device_map= {"": 0} # L4 이상 GRU 사용시
)

### 토크나이저 불러오기
tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

"""### 3. 인터페이스 데이터 정의"""

class InDataset(BaseModel):
    input : str

pip install transformers

"""### 4. 예측용 함수 정의"""

from transformers import pipeline
import torch

# 모델 및 토크나이저 이름 설정
model_name = "LEEHEEWON/midTermModel"
tokenizer_name = "LEEHEEWON/midTermModel"

# pipeline 초기화 (전역 변수로 선언)
global pipe
pipe = pipeline(
    "text-generation",
    model=model_name,
    tokenizer=tokenizer_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

def extract_response_llama3(instruction, input_text="", system_message="항상 답변 마지막에 '이상 의미있는 답변이었습니다'라고 말해."):
    user_content = instruction
    if input_text:
        user_content += f"\n{input_text}"
    messages = [
        {"role": "system", "content": system_message},
        {"role": "user", "content": user_content },
    ]
    print("vvvvvv")
    print(messages)
    prompt = pipe.tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    print("vvvvvv")
    print(prompt)

    terminators = [
        pipe.tokenizer.eos_token_id,
        pipe.tokenizer.convert_tokens_to_ids("<|eot_id|>")
    ]
    print("vvvvvv")
    print(terminators)
    outputs = pipe(
        prompt,
        max_new_tokens=256,
        eos_token_id=terminators,
        do_sample=True,
        temperature=0.1,
        top_p=0.9,
        num_return_sequences=1,
        return_full_text=False # True 시 질문까지 가져옴
    )
    print("vvvvvv")

    print(outputs)

    generated_text = outputs[0]['generated_text'].strip()
    return generated_text

@app.post("/generalKnowledge", status_code=200)
async def general_kn(x: InDataset):
    print(x)
    inTargetquestion = x.input
    print(inTargetquestion)
    ###### 입력값 활용 로직 구현 ######
    response = extract_response_llama3(inTargetquestion)
    return {"text": response }

@app.get('/')
async def root():
    return {"message": "online"}

pip show uvicorn
pip show fastapi
pip show nest-asyncio
pip show pyngrok
pip show pandas
pip show transformers

"""### 5. 서버오픈"""

# if __name__ == "__main__":
#     uvicorn.run("app:app", host="0.0.0.0", port=9999, log_level="debug",
#                 proxy_headers=True, reload=True)

auth_token = "2wkTzdluHL1riDZIiWFYurTiWGU_6uBZCeodyPqa2CTaheJbP"
ngrok.set_auth_token(auth_token)
ngrokTunnel = ngrok.connect(9999)
print("공용 URL", ngrokTunnel.public_url)
nest_asyncio.apply()
uvicorn.run(app, port=9999)