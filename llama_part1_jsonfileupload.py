# -*- coding: utf-8 -*-
"""llama_part1_jsonFileUpload.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A93rtI_9JDnEdl1g6hJ7J3HXFj4dInm1
"""

pip install datasets

import requests
import json
import pandas as pd
from datasets import Dataset
import huggingface_hub

url = "https://drive.google.com/uc?export=download&id=1lvguL6vSKlqCFSjkrGd1f7xyagD_Rnzc" ## 데이터를 수집후 구글 드라이브에 올린 후 반드시 공유에서 제한됨을 풀어줘야된다.
response = requests.get(url) ## json파일

original_data = json.loads(response.text)
print(original_data)

original_data.keys()

datas = original_data["data"]
len(datas)

eachData = datas[0]
eachData

eachData.keys()

eachparagraphs = eachData["paragraphs"][0]
eachparagraphs

eachQas = eachparagraphs["qas"][0]
eachQas

eachQuestion = eachQas["question"]
eachQuestion

eachText = eachQas["answers"][0]["text"]
eachText

eachContext = eachData['paragraphs'][0]['context']
eachContext

import pandas as pd
import json


contextList = []
questionList = []
AnswerList = []

for i in range(len(datas)):
    eachData = datas[i]
    eachparagraphs = eachData["paragraphs"]

    for j in range(len(eachparagraphs)):
        eachParagraph = eachparagraphs[j]
        eachQas = eachParagraph["qas"]

        for t in range(len(eachQas)):
            eachQa = eachQas[t]
            eachContext = eachParagraph['context']  # 수정된 부분
            eachQuestion = eachQa["question"]
            eachText = eachQa["answers"][0]["text"]
            contextList.append(eachContext)
            questionList.append(eachQuestion)
            AnswerList.append(eachText)
alphaca = pd.DataFrame(zip(contextList, questionList, AnswerList), columns=("instruction", "input", "output"))
alphaca





# Dataset 형태로 변환
dataset = Dataset.from_pandas(alphaca)

# 허깅페이스 로그인 ( https://huggingface.co/ 회원가입 후 DATASET 하나 추가 이후 API 키 생성)
huggingface_hub.login()

# 허깅페이스에 업로드 ( 본인의 데이터셋 생성한 주소를 적어야함)
dataset.push_to_hub("LEEHEEWON/midTermModel")

